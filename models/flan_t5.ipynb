{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!python -m pip install --upgrade pip\n","!pip install transformers huggingface_hub datasets evaluate rouge-score py7zr nltk wandb\n","!sudo apt-get install git-lfs --yes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import wandb\n","\n","dataset_id = \"samsum\"\n","model_id = \"google/flan-t5-small\"\n","model_name = f\"{model_id.split('/')[1]}-{dataset_id}\"\n","\n","config = {\n","    'num_epochs': 1,\n","    'batch_size': 16,\n","    'lr': 1e-5,\n","}\n","\n","run = wandb.init(\n","    mode='run', #run/disabled\n","    entity='hf-qa-bot',\n","    project='qa_model',\n","    name='run0',\n","    config=config\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from random import randrange\n","from datasets import load_dataset\n","\n","dataset = load_dataset(dataset_id)\n","dataset['train'] = dataset['train'].select(range(500))\n","\n","print(f\"train size: {len(dataset['train'])} test size: {len(dataset['test'])}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample = dataset['train'][randrange(len(dataset[\"train\"]))]\n","print(f\"dialogue: \\n{sample['dialogue']}\\n\", 20*\"-\")\n","print(f\"summary: \\n{sample['summary']}\\n\", 20*\"-\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from datasets import concatenate_datasets\n","\n","# The maximum total input sequence length after tokenization.\n","# Sequences longer than this will be truncated, sequences shorter will be padded.\n","tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n","max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n","print(f\"Max source length: {max_source_length}\")\n","\n","# The maximum total sequence length for target text after tokenization.\n","# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n","tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n","max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n","print(f\"Max target length: {max_target_length}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def preprocess_function(sample,padding=\"max_length\"):\n","    # add prefix to the input for t5\n","    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n","\n","    # tokenize inputs\n","    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n","\n","    # Tokenize targets with the `text_target` keyword argument\n","    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n","\n","    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n","    # padding in the loss.\n","    if padding == \"max_length\":\n","        labels[\"input_ids\"] = [\n","            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n","        ]\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n","print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import evaluate\n","import nltk\n","import numpy as np\n","from nltk.tokenize import sent_tokenize\n","nltk.download(\"punkt\")\n","\n","metric = evaluate.load(\"rouge\")\n","\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [label.strip() for label in labels]\n","\n","    # rougeLSum expects newline after each sentence\n","    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n","    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n","    return preds, labels\n","\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Some simple post-processing\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    result = {k: round(v * 100, 4) for k, v in result.items()}\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","# we want to ignore tokenizer pad token in the loss\n","label_pad_token_id = -100\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer,\n","    model=model,\n","    label_pad_token_id=label_pad_token_id,\n","    pad_to_multiple_of=8\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from huggingface_hub import HfFolder\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","\n","output_dir = model_name\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=output_dir,\n","    overwrite_output_dir=True,\n","    num_train_epochs=config['num_epochs'],\n","    per_device_train_batch_size=config['batch_size'],\n","    per_device_eval_batch_size=config['batch_size'],\n","    learning_rate=config['lr'],\n","    predict_with_generate=True,\n","    fp16=False,\n","    # logging & evaluation strategies\n","    logging_dir=f\"./{output_dir}/logs\",\n","    logging_strategy=\"steps\",\n","    logging_steps=10,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    save_total_limit=2,\n","    load_best_model_at_end=True,\n","    # push to hub parameters\n","    report_to=\"wandb\",\n","    push_to_hub=True,\n","    hub_strategy=\"every_save\",\n","    hub_model_id=output_dir,\n","    hub_token=HfFolder.get_token(),\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"test\"],\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import logging\n","logging.disable(logging.INFO)\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if config['num_epochs']:\n","    tokenizer.save_pretrained(output_dir)\n","    trainer.create_model_card()\n","    trainer.push_to_hub(model_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import pipeline\n","from random import randrange\n","\n","model =  f\"KonradSzafer/{model_name}\"\n","summarizer = pipeline(\"summarization\", model=model, device=0)\n","\n","sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n","print(f\"dialogue: \\n{sample['dialogue']}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["res = summarizer(sample[\"dialogue\"])\n","print(f\"flan-t5-base summary:\\n{res[0]['summary_text']}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["at = wandb.Artifact(\"test_predictions\", type=\"test_samples\")\n","text_table = wandb.Table(columns=[\"text\", \"oryginal\", \"predicted\"])\n","\n","for i in range(10):\n","    sample = dataset['test'][i]\n","    dialogue = sample['dialogue']\n","    oryginal = sample['summary']\n","    prediction = summarizer(sample[\"dialogue\"])\n","    prediction = prediction[0]['summary_text']\n","    text_table.add_data(dialogue, oryginal, prediction) \n","\n","at.add(text_table, \"predictions\")\n","run.log_artifact(at)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
